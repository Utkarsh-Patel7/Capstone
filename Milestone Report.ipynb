{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictive Keyboard. Milestone report\n",
    "=======================================\n",
    "\n",
    "The project, in general, is an exercise in building a predictive model for text input, using a keyboard. The predictive model could be a combination of probabilistic models (N-grams, others), and rule-based models (which, in general, could *also* be modeled using probabilities). For various tasks of the keyboard, different models will be used. Since this document is just a milestone to check how the progress is going, I'll reserve the more detailed discussion for the final document. \n",
    "\n",
    "\n",
    "Deliverables for this milestone\n",
    "-------------------------------\n",
    "\n",
    "The main deliverables are: \n",
    "- Demonstrate that you've downloaded the data and have successfully loaded it in.\n",
    "- Create a basic report of summary statistics about the data sets.\n",
    "- Report any interesting findings that you amassed so far.\n",
    "\n",
    "\n",
    "What data do we have?  \n",
    "---------------------\n",
    "\n",
    "The data provided consists of 4 sets of files, containing samples of tweets, blogs and news, in English, German, Finnish and Russian. Some basic data statistics follow:lines, word counts, etc (used *wc* command, recursively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "html"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "  lines       words     file\n",
    " 371440    12653185    .//de_DE/de_DE.blogs.txt\n",
    " 244743    13219388    .//de_DE/de_DE.news.txt\n",
    " 947774    11803735    .//de_DE/de_DE.twitter.txt\n",
    " \n",
    " 899288    37334690    .//en_US/en_US.blogs.txt\n",
    "1010242    34372720    .//en_US/en_US.news.txt\n",
    "2360148    30374206    .//en_US/en_US.twitter.txt\n",
    " \n",
    " 439785    12732013    .//fi_FI/fi_FI.blogs.txt\n",
    " 485758    10446725    .//fi_FI/fi_FI.news.txt\n",
    " 285214     3153003    .//fi_FI/fi_FI.twitter.txt\n",
    " \n",
    " 337100     9691167    .//ru_RU/ru_RU.blogs.txt\n",
    " 196360     9416099    .//ru_RU/ru_RU.news.txt\n",
    " 881414     9542485    .//ru_RU/ru_RU.twitter.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it seems we have a large amount of data to analyze. \n",
    "\n",
    "\n",
    "Exploratory analysis\n",
    "-----------------------------\n",
    "\n",
    "...create a basic report of summary statistics about the data sets. Basically at this point we want a quick N-grams analysis, with Uni and Bigrams, and check the frequencies of the most used words or expressions. I'll use mostly the **tm** (text mining) and **RWeka** libraries for the initial exploration, I'll start with a subset of the blogs set, in English. The R code should be run from the parent folder of the languages folders (so the folder containing the *data/en_US* folder). \n",
    "I'll use a small subset of data for this initial exploratory task (several issues with the RWeka, Weka and Java on Mac - error \"Error in rep(seq_along(x), sapply(tflist, length)) : invalid 'times' argument\", which forced me to use a single core on the Mac: options(mc.cores=1) - more details here http://stackoverflow.com/questions/17703553/bigrams-instead-of-single-words-in-termdocument-matrix-using-r-and-rweka))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library(tm)\n",
    "library(RWeka)\n",
    "\n",
    "## create a UnigramTokenizer (RWeka)\n",
    "UnigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))\n",
    "## create a BigramTokenizer (RWeka)\n",
    "BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))\n",
    "\n",
    "## load the english documents\n",
    "en_texts <- VCorpus(DirSource(directory=\"data/en_US/small\", encoding=\"UTF-8\"), \n",
    "                              readerControl=list(language=\"en\"))\n",
    "\n",
    "## get rid of extra white spaces, stopwords, DON'T STEM YET, switch to lowercase\n",
    "en_texts <- tm_map(x=en_texts, FUN=removePunctuation)\n",
    "en_texts <- tm_map(x=en_texts, FUN=removeWords, words=stopwords(kind=\"en\"))\n",
    "en_texts <- tm_map(x=en_texts, FUN=stripWhitespace)\n",
    "en_texts <- tm_map(x=en_texts, FUN=tolower)\n",
    "\n",
    "## create a TermDocumentMatrix  \n",
    "## NOTE - without the \"options\" underneath, the TermDocumentMatrix call crashes - \n",
    "## (looks like a parallel processing issue)\n",
    "options(mc.cores=1)\n",
    "tdmUnigram <- TermDocumentMatrix(en_texts, control=list(tokenizer=UnigramTokenizer))\n",
    "tdmBigram <- TermDocumentMatrix(en_texts, control=list(tokenizer=BigramTokenizer))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
